{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with CFG representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import tqdm\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from spektral.datasets import delaunay\n",
    "from spektral.layers import *\n",
    "from spektral.utils.convolution import localpooling_filter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "\n",
    "from utils import yaml_load, get_section, GraphDataGenerator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining some parameters for training\n",
    "N = 300                         # Number of nodes in the graphs\n",
    "F = 67                          # Original feature dimensionality\n",
    "n_classes = 4                   # Number of classes\n",
    "epochs = 200                    # Number of training epochs\n",
    "learning_rate = 1e-2            # Learning rate\n",
    "batch_size = 16                 # batch_size\n",
    "train_size = 0.85\n",
    "validation_size = 0.05\n",
    "test_size = 0.10\n",
    "\n",
    "data_root = './data/cfgs/'\n",
    "labels_file = './data/cfgs/labels.yaml'\n",
    "labels = yaml_load(labels_file)\n",
    "filenames = list(labels.keys())\n",
    "random.shuffle(filenames)\n",
    "\n",
    "train_files = filenames[0 : 0+int(len(filenames)*train_size)]\n",
    "validation_files = filenames[int(len(filenames)*train_size) : int(len(filenames)*train_size)+int(len(filenames)*validation_size)]\n",
    "test_files = filenames[int(len(filenames)*validation_size) : int(len(filenames)*validation_size)+int(len(filenames)*test_size)]\n",
    "\n",
    "train_generator = GraphDataGenerator(data_root, train_files, labels, batch_size=batch_size)\n",
    "validation_generator = GraphDataGenerator(data_root, validation_files, labels, batch_size=batch_size)\n",
    "test_generator = GraphDataGenerator(data_root, test_files, labels, batch_size=batch_size)\n",
    "\n",
    "print(f\"\"\"Samples:\n",
    "Total samples: {len(filenames)}\n",
    "Train samples: {len(train_files)}\n",
    "Validation samples: {len(validation_files)}\n",
    "Test samples: {len(test_files)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "conv   = GraphConv(45,activation='relu',use_bias=False)\n",
    "mincut = MinCutPool(N // 2)\n",
    "conv2  = GraphConv(55,activation='relu',use_bias=False)\n",
    "pool   = GlobalAttnSumPool()\n",
    "\n",
    "# First Graph Layers\n",
    "X1_in = Input(shape=(N, F))\n",
    "A1_in = Input((N, N))\n",
    "gc2_1 = conv([X1_in, A1_in])\n",
    "gc2_1, A1 = mincut([gc2_1,A1_in])\n",
    "gc2_1 = conv2([gc2_1, A1])\n",
    "pool_1 =  pool(gc2_1)\n",
    "d1 = Dense(200,activation='relu')(pool_1)\n",
    "\n",
    "# Second Graph Layers\n",
    "X2_in = Input(shape=(N, F))\n",
    "A2_in = Input((N, N))\n",
    "gc2_2 = conv([X2_in, A2_in]) # Notice that both graphs shares layers (shared weights)\n",
    "gc2_2, A2 = mincut([gc2_2,A2_in])\n",
    "gc2_2 = conv2([gc2_2, A2])\n",
    "pool_2 = pool(gc2_2)\n",
    "d2 = Dense(200,activation='relu')(pool_2)\n",
    "\n",
    "# Dense final layers\n",
    "merged = Concatenate()([d1, d2])\n",
    "\n",
    "merged1 = Dense(800,activation='relu')(merged)\n",
    "merged2 = Dense(32,activation='relu')(merged1)\n",
    "\n",
    "classe = Dense(n_classes, name=\"class\",activation=\"softmax\")(merged2)\n",
    "speedup = Dense(1, name=\"speddup\")(merged2)\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X1_in, A1_in,X2_in, A2_in], outputs=[classe,speedup])\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=['categorical_crossentropy', 'mse'],loss_weights=[1, 0.00005], weighted_metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "#history = model.fit([x_train[:,0,:,:], A_train[:,0,:,:], x_train[:,1,:,:], A_train[:,1,:,:]], [y_train[:,0:4], y_train[:,4]],\n",
    "#          batch_size=batch_size, validation_split=0.05, epochs=epochs)\n",
    "history = model.fit(train_generator, validation_data=validation_generator, epochs=epochs)\n",
    "\n",
    "plt.plot(np.array(history.history['class_acc']))\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.xlabel('Época')\n",
    "plt.legend(['treino', 'val'], loc='upper left')\n",
    "plt.savefig(f\"figures/GNN_train.cfg.epochs_{epochs}.batch_{batch_size}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print('Evaluating model.')\n",
    "#eval_results = model.evaluate([x_test[:,0,:,:], A_test[:,0,:,:], x_test[:,1,:,:], A_test[:,1,:,:]], [y_test[:,0:4],y_test[:,4]],\n",
    "#                              batch_size=batch_size)\n",
    "eval_results = model.evaluate(test_generator)\n",
    "print('Done. Test loss: {:.4f}. Test acc: {:.2f}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "# pred_vals = model.predict([x_test[:,0,:,:], A_test[:,0,:,:], x_test[:,1,:,:], A_test[:,1,:,:]])\n",
    "pred_vals = model.predict(test_generator)\n",
    "pred_vals = np.argmax(pred_vals[0], axis=-1)\n",
    "\n",
    "test_values = []\n",
    "for i in range(0, len(test_generator)):\n",
    "    _, (classe, _) = test_generator[i]\n",
    "    for j in range(0, test_generator.batch_size):\n",
    "        test_values.append(classe[j])\n",
    "test_values = np.stack(test_values)    \n",
    "test_values = np.argmax(test_values, axis=-1)\n",
    "\n",
    "cm=confusion_matrix(pred_vals,test_values)\n",
    "print(cm)\n",
    "\n",
    "labels = ['High Slowdown', 'Low Slowdown', 'Not Significant', 'High Speedup']\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)\n",
    "ax = sn.heatmap(cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16}, fmt='.2f', xticklabels=labels, yticklabels=labels)\n",
    "for label in ax.get_xticklabels():\n",
    "    label.set_ha(\"right\")\n",
    "    label.set_rotation(45)\n",
    "\n",
    "ax.set_xlabel('Truth Category')\n",
    "ax.set_ylabel('Predicted Category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, we save the trained network for further reuse (model configuration and weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving weights\n",
    "output_model_file = f'./model_data/ccpe_cfg_GNN.trained.epochs_{epochs}.batch_{batch_size}.h5'\n",
    "model.save(output_model_file, overwrite=True)\n",
    "print(f'Model saved to {output_model_file}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
