{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate CFG Input Representations (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import glob\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from spektral.datasets import delaunay\n",
    "from spektral.layers import *\n",
    "from spektral.utils.convolution import localpooling_filter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tqdm.contrib.concurrent import process_map, thread_map\n",
    "\n",
    "from utils import yaml_load, get_section\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating CFG Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets filter graphs whose number of nodes is below a determined shape (indicated by network_input_graph_shape). Larger graphs will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs with adjacency matrix below shape (150, 150): 17092\n"
     ]
    }
   ],
   "source": [
    "# Defining some useful variables\n",
    "metadata_file = './data/ccpe-applications-information.yaml'\n",
    "network_input_graph_shape = (150, 150)\n",
    "network_input_features_shape = (150, 67)\n",
    "\n",
    "metadata_info = yaml_load(metadata_file)\n",
    "\n",
    "# Lets filter CFG files whose number of nodes is less than network_input_graph_shape\n",
    "# For example, only CFG graphs with less than 150 nodes. Note: the graphs are represented as adjacency matrix (a quadratic matrix)\n",
    "filtered_graph_files = {\n",
    "    benchmark_name: {\n",
    "        opt_seq: opt_values \n",
    "        for opt_seq, opt_values in values.items() if opt_values['number_cfg_nodes'] < network_input_graph_shape[0]\n",
    "    } for benchmark_name, values in metadata_info.items()\n",
    "}\n",
    "\n",
    "# Remove applications that has less than 2 graphs with less than network_input_graph_shape\n",
    "filtered_graph_files = {benchmark_name: values for benchmark_name, values in filtered_graph_files.items() if len(values) > 2}\n",
    "print(f\"Number of graphs with adjacency matrix below shape {network_input_graph_shape}: {sum([len(values) for _, values in filtered_graph_files.items()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we are going to sample graphs (indicated by 'no_samples') from the filtered_graphs (i.e. below a determined shape). \n",
    "\n",
    "To do this we randomly select 'no_samples' applications and then, for each application, we select 2 different graphs (with different optimization sequences). At end, we will have a list of tuples with the following format: \\[(CFGFILE1, CFGFILE2, SPEEDUP), ...\\].\n",
    "\n",
    "*Note*: The graphs are not loaded yet, only the file names are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 20000 CFGs\n",
      "First sample of 20000 samples (example of output): ('./data/ccpe-dados/cfg.llvm/extracted/Misc.flops.0.78.ll.cfg.yaml', './data/ccpe-dados/cfg.llvm/extracted/Misc.flops.0.63.ll.cfg.yaml', 0.844077694995504)\n"
     ]
    }
   ],
   "source": [
    "no_samples = 20000  # Number of samples to generate. Each sample is composed by 2 graphs\n",
    "samples = []\n",
    "\n",
    "# Lets choose 'no_samples' random CFG applications and then choose 2 different graphs from each application.\n",
    "# Note: graphs here are the files (the graphs is not loaded yet).\n",
    "# We will have a list of tuples: [(CFGFILE1, CFGFILE2, SPEEDUP), ...]\n",
    "for s in random.choices(list(filtered_graph_files.keys()), k=no_samples):\n",
    "    opt_seq1, opt_seq2 = random.sample(list(filtered_graph_files[s].keys()), 2)\n",
    "    t = (\n",
    "        filtered_graph_files[s][opt_seq1]['cfg_file'], \n",
    "        filtered_graph_files[s][opt_seq2]['cfg_file'],\n",
    "        filtered_graph_files[s][opt_seq1]['exectime']/filtered_graph_files[s][opt_seq2]['exectime']\n",
    "    )\n",
    "    samples.append(t)\n",
    "\n",
    "print(f\"Sampled {len(samples)} CFGs\")\n",
    "print(f\"First sample of {len(samples)} samples (example of output): {samples[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to load the selected graphs and create a np.array as input to our GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph and features from file\n",
    "def load_cfg_from_file(filename: str, network_shape: tuple, features_shape: tuple) -> tuple:\n",
    "    x = yaml_load(filename)\n",
    "    \n",
    "    # The graph is a list of adjacency. Lets transform it to an adjacency matrix\n",
    "    graph = np.full(network_shape, False)\n",
    "    for node, nodes_list in x['nodes'].items():\n",
    "        for n in nodes_list:\n",
    "            graph[node][n] = True\n",
    "\n",
    "    # Lets read the features from the graphs\n",
    "    features = np.zeros(features_shape)\n",
    "    for node, node_list in x['nodes_features'].items():\n",
    "        features[node] = node_list\n",
    "\n",
    "    return graph, features\n",
    "\n",
    "def load_sample(sample_tuple: tuple) -> tuple:\n",
    "    graph1, features1 = load_cfg_from_file(sample_tuple[0], network_input_graph_shape, network_input_features_shape)\n",
    "    graph2, features2 = load_cfg_from_file(sample_tuple[1], network_input_graph_shape, network_input_features_shape)\n",
    "    \n",
    "    graphs = np.empty((2, network_input_graph_shape[0], network_input_graph_shape[1]))\n",
    "    graphs[0] = graph1\n",
    "    graphs[1] = graph2\n",
    "\n",
    "    features = np.empty((2, network_input_features_shape[0], network_input_features_shape[1]))\n",
    "    features[0] = features1\n",
    "    features[1] = features2\n",
    "\n",
    "    # Returns:\n",
    "    # * A matrix with 2 graphs with shape: (2, network_input_graph_shape[0], network_input_graph_shape[1])\n",
    "    # * A matrix with 2 features with shape: (2, network_input_features_shape[0], network_input_features_shape[1])\n",
    "    # * A float with speedup of CFG1/CFG2\n",
    "    return graphs, features, sample_tuple[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [18:58<00:00, 17.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 20000 samples\n",
      "Graphs shape: (20000, 2, 150, 150)\n",
      "Features shape: (20000, 2, 150, 67)\n",
      "Speedup shape: (20000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_graphs = np.empty((no_samples, 2, network_input_graph_shape[0], network_input_graph_shape[1]))\n",
    "input_features = np.empty((no_samples, 2, network_input_features_shape[0], network_input_features_shape[1]))\n",
    "speedups = np.empty((no_samples, 1))\n",
    "i = 0\n",
    "for sample in tqdm.tqdm(samples):\n",
    "    graphs, features, speedup = load_sample(sample)\n",
    "    input_graphs[i] = graphs\n",
    "    input_features[i] = features\n",
    "    speedups[i] = speedup\n",
    "    i+=1\n",
    "    \n",
    "print(f\"Generated {i} samples\")\n",
    "print(f\"Graphs shape: {input_graphs.shape}\")\n",
    "print(f\"Features shape: {input_features.shape}\")\n",
    "print(f\"Speedup shape: {speedups.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/cfgs_20000samples_150x150.npz\n",
      "Samples information saved to ./data/selected_cfgs_20000samples_150x150.yaml\n"
     ]
    }
   ],
   "source": [
    "output_data_file = f'./data/cfgs_{no_samples}samples_{network_input_graph_shape[0]}x{network_input_graph_shape[1]}'\n",
    "np.savez_compressed(output_data_file, graphs=input_graphs, features=input_features, speedups=speedups)\n",
    "print(f\"Data saved to {output_data_file}.npz\")\n",
    "\n",
    "output_samples_file = f'./data/selected_cfgs_{no_samples}samples_{network_input_graph_shape[0]}x{network_input_graph_shape[1]}.yaml'\n",
    "with open(output_samples_file, 'wt') as f:\n",
    "    yaml.dump(samples, f)\n",
    "print(f\"Samples information saved to {output_samples_file}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output_dir = './data/cfgs/'\n",
    "shutil.rmtree(output_dir, ignore_errors=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "labels_file = os.path.join(output_dir, 'labels.yaml')\n",
    "samples_file = os.path.join(output_dir, 'cfgs_selected.yaml')\n",
    "speedups = {}\n",
    "i = 0\n",
    "\n",
    "for sample in tqdm.tqdm(samples):\n",
    "    cfg_name = f'cfg-{i}'\n",
    "    filename = os.path.join(output_dir, cfg_name)\n",
    "    graphs, features, speedup = load_sample(sample)\n",
    "    np.savez_compressed(filename, graph=graphs, feature=features)\n",
    "    speedups[cfg_name] = speedup\n",
    "    i += 1\n",
    "print(f\"{i} samples saved to {output_dir}\")\n",
    "\n",
    "with open(labels_file, 'wt') as f:\n",
    "    yaml.dump(speedups, f)\n",
    "print(f\"Labels saved to {labels_file}\")\n",
    "\n",
    "with open(samples_file, 'wt') as f:\n",
    "    yaml.dump(samples, f)    \n",
    "print(f\"Samples information saved to {samples_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
